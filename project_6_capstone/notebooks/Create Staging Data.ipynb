{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T19:39:10.317940Z",
     "start_time": "2020-08-31T19:39:09.682918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'data-exploration', 'conf': {'spark.driver.memory': '4G', 'spark.jars.packages': 'saurfang:spark-sas7bdat:2.1.0-s_2.11', 'spark.driver.extraJavaOptions': '-Dlog4jspark.root.logger=DEBUG,console'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \n",
    "    \"name\": \"data-exploration\",\n",
    "    \"conf\": {\n",
    "        \"spark.driver.memory\": \"4G\",\n",
    "        \"spark.jars.packages\": \"saurfang:spark-sas7bdat:2.1.0-s_2.11\", \n",
    "        \"spark.driver.extraJavaOptions\" : \"-Dlog4jspark.root.logger=DEBUG,console\" \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T21:23:29.612455Z",
     "start_time": "2020-08-31T21:23:28.890686Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://localhost:8998/sessions/1 with error payload: {\"msg\":\"Session '1' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DateType, BooleanType\n",
    "\n",
    "\n",
    "@udf(DateType())\n",
    "def get_date_from_int(x):\n",
    "    if x is None:\n",
    "        return x\n",
    "    if x < 0:\n",
    "        return datetime.datetime(1900, 1, 1)  # default date for invalid dates\n",
    "    else:\n",
    "        return datetime.datetime(1960, 1, 1) + datetime.timedelta(x)\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def normalize_gender(x):\n",
    "    if x in {\"M\", \"F\"}:\n",
    "        return x\n",
    "    elif x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return \"OTHER\"\n",
    "\n",
    "\n",
    "@udf(BooleanType())\n",
    "def normalize_match_flag(x):\n",
    "    if x == None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def normalize_mode(x):\n",
    "    if x == 1:\n",
    "        return \"Air\"\n",
    "    elif x == 2:\n",
    "        return \"Sea\"\n",
    "    elif x == 3:\n",
    "        return \"Land\"\n",
    "    elif x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def normalize_visa_category(x):\n",
    "    if x == 1:\n",
    "        return \"Business\"\n",
    "    elif x == 2:\n",
    "        return \"Pleasure\"\n",
    "    elif x == 3:\n",
    "        return \"Student\"\n",
    "    elif x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return \"Other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T19:53:59.710173Z",
     "start_time": "2020-08-31T19:53:58.283621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "import os\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType\n",
    "\n",
    "\n",
    "VALID_COLUMNS = [\n",
    "    'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate',\n",
    "    'i94bir', 'i94visa', 'count', 'dtadfile',\n",
    "    'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear',\n",
    "    'dtaddto', 'gender', 'insnum', 'airline', 'admnum', 'fltno', 'visatype'\n",
    "]\n",
    "\n",
    "\n",
    "FINAL_COLUMNS = [\n",
    "    \"year\", \"month\", \"day\",\n",
    "    \"arrival_date\", \"departure_date\", \"permitted_date\", \"unrestricted_stay\",\n",
    "    \"country_citizenship\", \"country_residence\", \"port_name\",\n",
    "    \"destination_state\",\n",
    "    \"age\", \"gender\", \"num_previous_stays\",\n",
    "    \"visa_type\", \"visa_category\",\n",
    "    \"is_overstay\"\n",
    "]\n",
    "\n",
    "SAS_FORMAT_READER = \"com.github.saurfang.sas.spark\"\n",
    "\n",
    "\n",
    "\n",
    "def read_sas_data(spark, path: str, columns_to_read=None):\n",
    "\n",
    "    logging.info(\"Reading the immigration data\")\n",
    "\n",
    "    url_components = urlparse(path)\n",
    "\n",
    "    if url_components.scheme == 's3':\n",
    "        s3_client = boto3.client('s3')\n",
    "\n",
    "        s3_objects = s3_client.list_objects(Bucket=url_components.netloc, Prefix=url_components.path.lstrip(\"/\"))\n",
    "\n",
    "        df = None\n",
    "        for key in s3_objects['Contents']:\n",
    "            file_key = key['Key']\n",
    "            print(\"Reading File: {}\".format(file_key))\n",
    "            logging.warning(\"Reading File: {}\".format(file_key))\n",
    "            file_df = (\n",
    "                spark.read\n",
    "                .format(SAS_FORMAT_READER)\n",
    "                .load(\"s3://{}/{}\".format(url_components.netloc, file_key))\n",
    "            )\n",
    "            if columns_to_read:\n",
    "                file_df = file_df.select(VALID_COLUMNS)\n",
    "            if df:\n",
    "                df = df.union(file_df)\n",
    "            else:\n",
    "                df = file_df\n",
    "    else:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(SAS_FORMAT_READER)\n",
    "            .load(\"s3://{}/{}\".format(url_components.netloc, file_key))\n",
    "        )\n",
    "        if columns_to_read:\n",
    "            df.select(columns_to_read)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_dimension_data(spark: SparkSession, root_path: str):\n",
    "\n",
    "    logging.info(\"Reading the dimension data\")\n",
    "\n",
    "    gdp_df = spark.read.option(\"header\", True).format(\"csv\").load(f\"{root_path}/gdp.csv\")\n",
    "    country_ids_df = spark.read.option(\"header\", True).format(\"csv\").load(f\"{root_path}/country_ids.csv\")\n",
    "    states_df = spark.read.option(\"header\", True).format(\"csv\").load(f\"{root_path}/states.csv\")\n",
    "    ports_df = spark.read.option(\"header\", True).format(\"csv\").load(f\"{root_path}/ports.csv\")\n",
    "\n",
    "    country_ids_df_cleaned = (\n",
    "        country_ids_df\n",
    "        .withColumn(\"country_id\", F.col(\"country_id\").cast(IntegerType()))\n",
    "        .withColumn(\"country_name\", F.initcap(F.col(\"country_name\")))  # title case\n",
    "        .withColumn(\"country_name_lower\", F.lower(F.col(\"country_name\")))\n",
    "    )\n",
    "    gdp_df_cleaned = (\n",
    "        gdp_df\n",
    "        .withColumnRenamed(\"Country Name\", \"country_name\")\n",
    "        .withColumnRenamed(\"Country Code\", \"country_code\")\n",
    "        .withColumnRenamed(\"Value\", \"gdp_value\")\n",
    "        # .filter(F.col(\"Year\") == \"2016\")\n",
    "        .withColumn(\"gdp_value\", F.col(\"gdp_value\").cast(FloatType()).cast(LongType()))\n",
    "        .withColumn(\"country_name\", F.initcap(F.col(\"country_name\")))  # title case\n",
    "        .withColumn(\"country_name_lower\", F.lower(F.col(\"country_name\")))\n",
    "        .drop(\"Year\")\n",
    "    )\n",
    "\n",
    "    return country_ids_df_cleaned, gdp_df_cleaned, ports_df\n",
    "\n",
    "\n",
    "def clean_immigration_data(raw_data):\n",
    "\n",
    "    logging.warning(\"Cleaning the immigration data\")\n",
    "\n",
    "    USEFUL_COLUMNS = [\n",
    "        \"admnum\", \"i94yr\", \"i94mon\",\n",
    "        \"arrdate\", \"depdate\", \"dtaddto\",\n",
    "        \"i94cit\", \"i94res\", \"i94port\", \"i94addr\",\n",
    "        \"matflag\", \"i94mode\",\n",
    "        \"i94bir\", \"gender\", \"visatype\", \"i94visa\",\n",
    "        \"entdepa\", \"entdepu\", \"entdepd\"\n",
    "    ]\n",
    "\n",
    "    clean_df = raw_data.select(USEFUL_COLUMNS)\n",
    "\n",
    "    clean_df = (\n",
    "        clean_df\n",
    "        .withColumnRenamed('i94yr', 'year')\n",
    "        .withColumnRenamed('i94mon', 'month')\n",
    "        .withColumnRenamed('i94res', 'country_residence_id')\n",
    "        .withColumnRenamed('i94cit', 'country_citizenship_id')\n",
    "        .withColumnRenamed('i94port', 'entry_port')\n",
    "        .withColumnRenamed('i94addr', 'destination_state')\n",
    "        .withColumnRenamed('i94mode', 'travel_mode')\n",
    "        .withColumnRenamed('i94bir', 'age')\n",
    "        .withColumnRenamed('i94visa', 'visa_category')\n",
    "        .withColumnRenamed('visatype', 'visa_type')\n",
    "        .withColumnRenamed('matflag', 'dates_match_flag')\n",
    "        .withColumnRenamed('arrdate', 'arrival_date')\n",
    "        .withColumnRenamed('depdate', 'departure_date')\n",
    "        .withColumnRenamed('dtaddto', 'permitted_date')\n",
    "    )\n",
    "\n",
    "    same_user_window = (\n",
    "        Window\n",
    "        .partitionBy(\"admnum\")\n",
    "        .orderBy(\"arrival_date\")\n",
    "        .rowsBetween(Window.unboundedPreceding, -1)\n",
    "    )\n",
    "    clean_df = (\n",
    "        clean_df\n",
    "        .withColumn('admnum', F.col(\"admnum\").cast(IntegerType()))\n",
    "        .withColumn('year', F.col(\"year\").cast(IntegerType()))\n",
    "        .withColumn('month', F.col(\"month\").cast(IntegerType()))\n",
    "        .withColumn('unrestricted_stay', F.when(F.col('permitted_date') == 'D/S', True).otherwise(False))\n",
    "        .withColumn('arrival_date', get_date_from_int(F.col(\"arrival_date\")))\n",
    "        .withColumn('departure_date', get_date_from_int(F.col(\"departure_date\")))\n",
    "        .withColumn('permitted_date', F.to_date('permitted_date', 'MMddyyyy'))\n",
    "        .withColumn('day', F.dayofmonth(\"arrival_date\"))\n",
    "        .withColumn('age', F.col(\"age\").cast(IntegerType()))\n",
    "        .withColumn('gender', normalize_gender(F.col(\"gender\")))\n",
    "        .withColumn('dates_match_flag', normalize_match_flag(F.col(\"dates_match_flag\")))\n",
    "        .withColumn('travel_mode', normalize_mode(F.col(\"travel_mode\")))\n",
    "        .withColumn('visa_category', normalize_visa_category(F.col(\"visa_category\").cast(IntegerType())))\n",
    "        .withColumn('country_citizenship_id', F.col(\"country_citizenship_id\").cast(IntegerType()))\n",
    "        .withColumn('country_residence_id', F.col(\"country_residence_id\").cast(IntegerType()))\n",
    "        .withColumn('num_previous_stays', F.count(\"*\").over(same_user_window))\n",
    "    )\n",
    "\n",
    "    logging.info(\"Adding the ML Label\")\n",
    "    # add the label column for the ML problem, check the Data Exploration notebook for the explanation of why \n",
    "    # these conditions were chosen\n",
    "    clean_df = clean_df.withColumn(\n",
    "        \"is_overstay\",\n",
    "        F.when(\n",
    "            (\n",
    "                (clean_df.dates_match_flag == False) &\n",
    "                (clean_df.unrestricted_stay == False) &\n",
    "                ((F.isnull(clean_df.departure_date)) & ((F.isnull(clean_df.entdepd)))) &\n",
    "                (F.isnull(clean_df.entdepu))\n",
    "            ),\n",
    "            True\n",
    "        ).otherwise(False)\n",
    "    )\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T19:41:09.375376Z",
     "start_time": "2020-08-31T19:41:08.135055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RAW_BUCKET_URL = \"s3://udacity-capstone-raw-data\"\n",
    "STAGING_BUCKET_URL = \"s3://udacity-capstone-staging-data\"\n",
    "DATA_PATH = f\"{RAW_BUCKET_URL}/i94-data\"\n",
    "OUTPUT_DATA_PATH = STAGING_BUCKET_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T20:03:46.554653Z",
     "start_time": "2020-08-31T20:03:45.063223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "immigration_data_path = DATA_PATH\n",
    "dimension_root_path = RAW_BUCKET_URL\n",
    "output_path = OUTPUT_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T20:03:59.859114Z",
     "start_time": "2020-08-31T20:03:46.556620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading File: i94-data/i94_apr16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_aug16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_dec16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_feb16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_jan16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_jul16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_jun16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_mar16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_may16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_nov16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_oct16_sub.sas7bdat\n",
      "Reading File: i94-data/i94_sep16_sub.sas7bdat"
     ]
    }
   ],
   "source": [
    "raw_data = read_sas_data(spark, immigration_data_path, VALID_COLUMNS)\n",
    "clean_df = clean_immigration_data(raw_data)\n",
    "country_df, gdp_df, port_df = read_dimension_data(spark, dimension_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T20:04:01.572476Z",
     "start_time": "2020-08-31T20:03:59.867092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_df = (\n",
    "    clean_df\n",
    "    .alias(\"immigration\")\n",
    "    .join(country_df.alias(\"cit_df\"), F.col(\"immigration.country_citizenship_id\") == F.col(\"cit_df.country_id\"), \"leftouter\")\n",
    "    .join(country_df.alias(\"res_df\"), F.col(\"immigration.country_residence_id\") == F.col(\"res_df.country_id\"), \"leftouter\")\n",
    "    .join(port_df.alias(\"port_df\"), F.col(\"immigration.entry_port\") == F.col(\"port_df.port_id\"), \"leftouter\")\n",
    "    .selectExpr(\"immigration.*\", \"cit_df.country_name as country_citizenship\", \"res_df.country_name as country_residence\", \"port_df.port_name\")\n",
    ")\n",
    "\n",
    "joined_df = joined_df.select(FINAL_COLUMNS)\n",
    "\n",
    "joined_df = joined_df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T20:13:08.433622Z",
     "start_time": "2020-08-31T20:04:01.575019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "    joined_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")  # we partition by day so that if we want to run this every day, we don't overwrite the month partition\n",
    "    .parquet(os.path.join(output_path, \"immigration_data\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T20:15:00.149779Z",
     "start_time": "2020-08-31T20:14:48.547201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "    gdp_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(os.path.join(output_path, \"gdp_data\"))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
