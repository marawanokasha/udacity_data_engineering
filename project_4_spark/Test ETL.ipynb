{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='[ %(asctime)s ] %(filename)s(%(lineno)d) %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"sparkify-ETL\")  \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "#input_data = \"s3a://udacity-dend/\"\n",
    "# output_data = \"s3a://udacity-lesson3-project-bucket/\"\n",
    "input_data = \"./data/\"\n",
    "output_data = \"./output_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://69e4d9fdf1c7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkify-ETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd2cbd3fcc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "! unzip -o -q data/song-data.zip -d data\n",
    "! unzip -o -q data/log-data.zip -d data/log_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p ./output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from etl import process_song_data, process_log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2020-06-15 20:12:17,106 ] etl.py(35) INFO - Default Parallelism is 4\n",
      "[ 2020-06-15 20:12:17,110 ] etl.py(51) INFO - Reading raw Song data from source: ./data/song_data/*/*/*/*.json\n",
      "[ 2020-06-15 20:12:17,628 ] etl.py(56) INFO - Raw Song Data has 4 partitions\n",
      "[ 2020-06-15 20:12:17,812 ] etl.py(57) INFO - Partition size is [18, 18, 18, 17]\n",
      "[ 2020-06-15 20:12:17,815 ] etl.py(59) INFO - Extracting the song dimension table\n",
      "[ 2020-06-15 20:12:17,923 ] etl.py(64) INFO - Song dimension table has 4 memory partitions\n",
      "[ 2020-06-15 20:12:17,926 ] etl.py(66) INFO - Writing back the song dimension table partitioned by year and artist_id\n",
      "[ 2020-06-15 20:12:18,773 ] etl.py(77) INFO - Extracting the artist dimension table\n",
      "[ 2020-06-15 20:12:18,862 ] etl.py(89) INFO - Artist dimension table has 200 memory partitions\n",
      "[ 2020-06-15 20:12:23,118 ] etl.py(90) INFO - Artist dimension table partition size is [1, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 2, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[ 2020-06-15 20:12:23,122 ] etl.py(92) INFO - Repartitioning the artists table to reduce partition skewness\n",
      "[ 2020-06-15 20:12:23,169 ] etl.py(95) INFO - Artist dimension table has 5 memory partitions\n",
      "[ 2020-06-15 20:12:24,407 ] etl.py(96) INFO - Artist dimension table partition size is [16, 13, 14, 11, 15]\n",
      "[ 2020-06-15 20:12:24,411 ] etl.py(99) INFO - Writing back the artist dimension table\n",
      "[ 2020-06-15 20:12:25,822 ] etl.py(109) INFO - Finished processing the raw song data\n"
     ]
    }
   ],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2020-06-15 20:12:43,632 ] etl.py(121) INFO - Default Parallelism is 4\n",
      "[ 2020-06-15 20:12:43,636 ] etl.py(122) INFO - Reading Log data from source\n",
      "[ 2020-06-15 20:12:43,942 ] etl.py(127) INFO - Raw Log Data has 4 partitions\n",
      "[ 2020-06-15 20:12:44,409 ] etl.py(128) INFO - Partition size is [3512, 2585, 1485, 474]\n",
      "[ 2020-06-15 20:12:44,518 ] etl.py(133) INFO - Extracting the users dimension table\n",
      "[ 2020-06-15 20:12:44,617 ] etl.py(144) INFO - Users dimension table has 200 memory partitions\n",
      "[ 2020-06-15 20:12:49,402 ] etl.py(145) INFO - Users dimension table partition size is [0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 3, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0]\n",
      "[ 2020-06-15 20:12:49,406 ] etl.py(146) INFO - Writing back the users dimension table\n",
      "[ 2020-06-15 20:12:53,364 ] etl.py(162) INFO - Extracting the time dimension table\n",
      "[ 2020-06-15 20:12:53,559 ] etl.py(175) INFO - Time dimension table has 4 memory partitions\n",
      "[ 2020-06-15 20:12:54,840 ] etl.py(176) INFO - Time dimension table partition size is [3013, 2194, 1219, 394]\n",
      "[ 2020-06-15 20:12:54,844 ] etl.py(177) INFO - Writing back the time dimension table partitioned by year and month\n",
      "[ 2020-06-15 20:12:56,304 ] etl.py(187) INFO - Reading the song dimension table so we can correlate song plays to songs\n",
      "[ 2020-06-15 20:12:56,748 ] etl.py(192) INFO - Song dimension table has 4 partitions\n",
      "[ 2020-06-15 20:12:57,095 ] etl.py(193) INFO - Song dimension table partition size is [18, 18, 18, 17]\n",
      "[ 2020-06-15 20:12:57,099 ] etl.py(195) INFO - Extracting the songplays Fact table by joining the logs DF to the song dimension table\n",
      "[ 2020-06-15 20:12:57,353 ] etl.py(214) INFO - Songplays fact table has 200 memory partitions\n",
      "[ 2020-06-15 20:13:02,469 ] etl.py(215) INFO - Songplays fact table partition size is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ 2020-06-15 20:13:02,474 ] etl.py(216) INFO - Writing back the songplays fact table\n",
      "[ 2020-06-15 20:13:06,427 ] etl.py(225) INFO - Finished processing the raw log data\n"
     ]
    }
   ],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
